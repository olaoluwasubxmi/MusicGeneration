{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Activation, Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('donda.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary:  6\n"
     ]
    }
   ],
   "source": [
    " tokenizer = Tokenizer()\n",
    " tokenizer.fit_on_texts(text)\n",
    " vocab_size = len(tokenizer.word_counts) + 1\n",
    " print(\"Total Vocabulary: \", vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Row Count:  6168\n",
      "Training Data Count:  4934\n",
      "Test Data Coount:  1234\n"
     ]
    }
   ],
   "source": [
    " N = text.shape[0]\n",
    " print(\"Total Row Count: \", N)\n",
    " prop_train = 0.8\n",
    " train = int(N*prop_train)\n",
    " print(\"Training Data Count: \", train)\n",
    " test = N - train\n",
    " print(\"Test Data Coount: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 4\n"
     ]
    }
   ],
   "source": [
    "sequences, index_train, index_test = [], [], []\n",
    "count = 0\n",
    "for irow,line in enumerate(text):\n",
    "    #print(irow, line)\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]    \n",
    "    #print(encoded)\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        if irow < train:        \n",
    "            index_train.append(count)     \n",
    "        else:         \n",
    "            index_test.append(count)\n",
    "        count += 1\n",
    "print('Total Sequences: %d' % (len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 5\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % (max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 4)\n",
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "print(y.shape)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X_train, y_train, X_test, y_test = X[index_train], y[index_train],X[index_test],  y[index_test]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "#make it smaller for testing\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:100]\n",
    "y_test = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,\n",
    "                  input_length=1,\n",
    "                  dim_dense_embedding=10,\n",
    "                  hidden_unit_LSTM=5):\n",
    "    main_input = Input(shape=(input_length,),dtype='int32',name='main_input')\n",
    "    embedding = Embedding(vocab_size, dim_dense_embedding, \n",
    "                          input_length=input_length)(main_input)\n",
    "    x = LSTM(hidden_unit_LSTM)(embedding)\n",
    "    main_output = Dense(vocab_size, activation='softmax')(x)\n",
    "    model = Model(inputs=[main_input],\n",
    "                   outputs=[main_output])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 - 2s - loss: 1.7908 - accuracy: 0.0000e+00 - 2s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "1/1 - 0s - loss: 1.7858 - accuracy: 0.0000e+00 - 33ms/epoch - 33ms/step\n",
      "Epoch 3/20\n",
      "1/1 - 0s - loss: 1.7806 - accuracy: 0.0000e+00 - 30ms/epoch - 30ms/step\n",
      "Epoch 4/20\n",
      "1/1 - 0s - loss: 1.7755 - accuracy: 0.2500 - 36ms/epoch - 36ms/step\n",
      "Epoch 5/20\n",
      "1/1 - 0s - loss: 1.7702 - accuracy: 0.2500 - 32ms/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "1/1 - 0s - loss: 1.7647 - accuracy: 0.2500 - 33ms/epoch - 33ms/step\n",
      "Epoch 7/20\n",
      "1/1 - 0s - loss: 1.7591 - accuracy: 0.2500 - 27ms/epoch - 27ms/step\n",
      "Epoch 8/20\n",
      "1/1 - 0s - loss: 1.7533 - accuracy: 0.5000 - 30ms/epoch - 30ms/step\n",
      "Epoch 9/20\n",
      "1/1 - 0s - loss: 1.7473 - accuracy: 0.5000 - 27ms/epoch - 27ms/step\n",
      "Epoch 10/20\n",
      "1/1 - 0s - loss: 1.7411 - accuracy: 0.5000 - 28ms/epoch - 28ms/step\n",
      "Epoch 11/20\n",
      "1/1 - 0s - loss: 1.7345 - accuracy: 0.5000 - 31ms/epoch - 31ms/step\n",
      "Epoch 12/20\n",
      "1/1 - 0s - loss: 1.7277 - accuracy: 0.5000 - 29ms/epoch - 29ms/step\n",
      "Epoch 13/20\n",
      "1/1 - 0s - loss: 1.7204 - accuracy: 0.5000 - 29ms/epoch - 29ms/step\n",
      "Epoch 14/20\n",
      "1/1 - 0s - loss: 1.7128 - accuracy: 0.5000 - 29ms/epoch - 29ms/step\n",
      "Epoch 15/20\n",
      "1/1 - 0s - loss: 1.7048 - accuracy: 0.5000 - 29ms/epoch - 29ms/step\n",
      "Epoch 16/20\n",
      "1/1 - 0s - loss: 1.6963 - accuracy: 0.5000 - 28ms/epoch - 28ms/step\n",
      "Epoch 17/20\n",
      "1/1 - 0s - loss: 1.6874 - accuracy: 0.5000 - 29ms/epoch - 29ms/step\n",
      "Epoch 18/20\n",
      "1/1 - 0s - loss: 1.6779 - accuracy: 0.5000 - 34ms/epoch - 34ms/step\n",
      "Epoch 19/20\n",
      "1/1 - 0s - loss: 1.6678 - accuracy: 0.5000 - 30ms/epoch - 30ms/step\n",
      "Epoch 20/20\n",
      "1/1 - 0s - loss: 1.6571 - accuracy: 0.5000 - 31ms/epoch - 31ms/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size,\n",
    "                   input_length=X.shape[1],\n",
    "                   dim_dense_embedding=30,\n",
    "                   hidden_unit_LSTM=64)\n",
    " ##compile network\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', metrics=['accuracy'])\n",
    " ##fit network\n",
    "tf_model = model.fit(X_train, y_train, \n",
    "                  validation_data = (X_test,y_test),\n",
    "                  epochs=20, verbose=2,\n",
    "                  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid bias shape: (2, 3072)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29436/559420998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'donda.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Subxmi\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Subxmi\\Anaconda3\\lib\\site-packages\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36m_convert_rnn_weights\u001b[1;34m(layer, weights)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"LSTM\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid bias shape: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert_lstm_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_cudnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid bias shape: (2, 3072)"
     ]
    }
   ],
   "source": [
    "model.load_weights('donda.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "285ddf65587f3c042e287c3856e7d92f4bb8cbaf58923bfd87e25b328822367e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
